{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#Import libraries \n",
    "import warnings, numpy as np, pandas as pan, matplotlib.pyplot as plt\n",
    "from pandas_datareader import data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always recommend that students and users import all of their libraries at the beginning of the file rather than importing them as they become necessary. Foremost, this is syntatically observed to be the case in most github repositories. Secondarily, this allows the code we write to be more legible\n",
    "\n",
    "We will begin this lecture by first defining a function that will utilize the KNN algorithm for classification in which the toy example converges on a feasible solution \n",
    "\n",
    "The following example will be one in which the algorithm will not converge on a feasible solution and in contrast will be a regression use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Shape: 150,4\n",
      "\n",
      "Description:\n",
      "                 0           1           2           3\n",
      "count  150.000000  150.000000  150.000000  150.000000\n",
      "mean     5.843333    3.057333    3.758000    1.199333\n",
      "std      0.828066    0.435866    1.765298    0.762238\n",
      "min      4.300000    2.000000    1.000000    0.100000\n",
      "25%      5.100000    2.800000    1.600000    0.300000\n",
      "50%      5.800000    3.000000    4.350000    1.300000\n",
      "75%      6.400000    3.300000    5.100000    1.800000\n",
      "max      7.900000    4.400000    6.900000    2.500000\n",
      "\n",
      "Correlation Coefficient Matrix:\n",
      "           0         1         2         3\n",
      "0  1.000000 -0.117570  0.871754  0.817941\n",
      "1 -0.117570  1.000000 -0.428440 -0.366126\n",
      "2  0.871754 -0.428440  1.000000  0.962865\n",
      "3  0.817941 -0.366126  0.962865  1.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2c0d22035ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel_iris_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2c0d22035ad7>\u001b[0m in \u001b[0;36mmodel_iris_data\u001b[0;34m(n_neighbors)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nKNN In Sample Accuracy w/ %s Neighbors: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nKNN In Sample F1 Score w/ %s Neighbors: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nIn Sample Confusion Matrix:\\n %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tawehbeysolow/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tawehbeysolow/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "def model_iris_data(n_neighbors):  \n",
    "    '''\n",
    "    This function loads the iris data from the sklearn datasets class, models this data set using KNN, \n",
    "    and then displays performance data on the algorithm (Classification)\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        n_neighbors - int - the number of neighbors to an observation KNN uses to output a discrete or continuous label\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    raw_iris_data = datasets.load_iris()\n",
    "    print('Data Set Shape: %s,%s'%(pan.DataFrame(raw_iris_data.data).shape))\n",
    "    print('\\nDescription:\\n %s'%pan.DataFrame(raw_iris_data.data).describe())        \n",
    "    print('\\nCorrelation Coefficient Matrix:\\n %s'%(pan.DataFrame(raw_iris_data.data).corr()))\n",
    "\n",
    "    x = pan.DataFrame(raw_iris_data.data[:, :4], columns=raw_iris_data.feature_names)\n",
    "    y = pan.DataFrame(raw_iris_data.target)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.33)\n",
    "\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=n_neighbors).fit(train_X, train_y)\n",
    "    predicted_labels = knn_model.predict(test_X)\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "    print('\\nKNN In Sample Accuracy w/ %s Neighbors: %s'%(n_neighbors, accuracy_score(test_y, predicted_labels)))\n",
    "    print('\\nKNN In Sample F1 Score w/ %s Neighbors: %s'%(n_neighbors, f1_score(test_y, predicted_labels, average='macro')))\n",
    "    print('\\nIn Sample Confusion Matrix:\\n %s'%(confusion_matrix(y, predicted_labels)))\n",
    "    \n",
    "    \n",
    "model_iris_data(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readers will observe that the above sample converged on a solution that yields an excellent accuracy rating with virtually no false predictions. This would be a solution that we should feel comfortable putting to further out of sample testing in the instance that we wanted to deploy this as a solution to this problem. \n",
    "\n",
    "With this being said, KNN is a toy dataset whose purpose is more to use as a proxy for understanding machine learning algorithms. The next example **will not** work, however the focus of the discussion will be for us to try and hypothesize why and what model might work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stock_data(n_neighbors, ticker_symbol, start_date, end_date, data_source='yahoo'):\n",
    "    '''\n",
    "    This function loads the stock data from yahoo finance, models this data set using KNN, \n",
    "    and then displays performance data on the algorithm (Regression). Time series is sampled daily\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        n_neighbors - int - the number of neighbors to an observation KNN uses to output a discrete or continuous label\n",
    "        ticker_symbol - str - the SPY ticker of the stock that is being modeled \n",
    "        start_date - str - initial sampling date of the time series (Format - YYYY-MM-DD)\n",
    "        end_date - str - final sampling date of the time series (Format - YYYY-MM-DD)\n",
    "        data_source - str - the data source from which the data is pulled (default: 'yahoo')\n",
    "        \n",
    "    '''\n",
    "     \n",
    "    stock_data = data.DataReader(name=ticker_symbol, \n",
    "                                 start=start_date, \n",
    "                                 end=end_date,\n",
    "                                 data_source=data_source)\n",
    "\n",
    "    x = stock_data.shift(1).dropna().reset_index(drop='index')\n",
    "    y = stock_data.Close.shift(-1).dropna().reset_index(drop='index')\n",
    "    train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=n_neighbors).fit(train_X, train_y)\n",
    "    predicted_labels = knn_model.predict(test_X)\n",
    "\n",
    "    plt.plot(predicted_labels, label='predicted price')\n",
    "    plt.plot(y, label='actual price')\n",
    "    plt.xlabel('N Days')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    print('KNN Mean Squared Error: %s'%mean_squared_error(test_y, predicted_labels))\n",
    "\n",
    "model_stock_data(n_neighbors=4,\n",
    "                 ticker_symbol='AAPL', \n",
    "                 start_date='2017-01-01', \n",
    "                 end_date='2019-01-01',\n",
    "                 data_source='yahoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the presented solution is exceptionally inaccurate and does not merit further testing if we were to try and deploy this model. Not only is the mean squared error exceptionally high, we can visually see that it is significantly underfitting the data. Why might this be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
